# -*- coding: utf-8 -*-
"""taim-tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gothCEA5O-a7JTA-S0LVZBtspeDknsvI

# Grab data from yfinance and show
"""

!pip install pattern
!pip install vaderSentiment
!pip install pattern3
!pip install yfinance

"""#helper"""

# Commented out IPython magic to ensure Python compatibility.
""" Plot functions 
    Helper!!

"""
# Reload modules before executing user code
# %load_ext autoreload
# Reload all modules every time before executing the Python code
# %autoreload 2
import pandas as pd
import matplotlib.pyplot as plt


def plot_crash_detections(
    start_date,
    end_date,
    threshold,
    distances,
    time_index_derivs,
    price_resampled_derivs,
    metric_name
):

    # calculate rolling mean, min, max of homological derivatives
    rolled_mean_h = pd.Series(distances).rolling(20, min_periods=1).mean()
    rolled_min_h = (
        pd.Series(distances)
        .rolling(len(distances), min_periods=1)
        .min()
    )
    rolled_max_h = (
        pd.Series(distances)
        .rolling(len(distances), min_periods=1)
        .max()
    )

    # normalise the time series values to lies within [0, 1]
    probability_of_crash_h = (rolled_mean_h - rolled_min_h) / (
        rolled_max_h - rolled_min_h
    )

    # define time intervals to plots
    is_date_in_interval = (time_index_derivs > pd.Timestamp(start_date)) & (
        time_index_derivs < pd.Timestamp(end_date)
    )
    probability_of_crash_h_region = probability_of_crash_h[is_date_in_interval]
    time_index_region = time_index_derivs[is_date_in_interval]
    resampled_close_price_region = price_resampled_derivs.loc[is_date_in_interval]

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    plt.plot(time_index_region, probability_of_crash_h_region, color="#1f77b4")
    plt.axhline(y=threshold, linewidth=2, color='#ff7f0e', linestyle='--', label='Threshold')
    plt.title(f"Crash Probability Based on {metric_name}")
    plt.legend(loc="best", prop={"size": 10},)

    plt.subplot(1, 2, 2)
    plt.plot(
        resampled_close_price_region[probability_of_crash_h_region.values > threshold],
        '#ff7f0e', marker='.', linestyle='None', markersize=4
    )
    plt.plot(
        resampled_close_price_region[probability_of_crash_h_region.values <= threshold],
        color="#1f77b4", marker='.', linestyle='None', markersize=4
    )

    plt.title("Close Price")
    plt.legend(
        [
            "Crash probability > {0}%".format(int(threshold * 100)),
            "Crash probability ≤ {0}%".format(int(threshold * 100)),
        ],
        loc="best",
        prop={"size": 10},
    )
    plt.savefig(f'./images/crash_{metric_name}.png')
    plt.show()
    
    
def plot_crash_comparisons(
    start_date,
    end_date,
    threshold,
    distances_1,
    distances_2,
    time_index_derivs,
    price_resampled_derivs,
):

    # calculate rolling mean, min, max of homological derivatives
    rolled_mean_1 = pd.Series(distances_1).rolling(20, min_periods=1).mean()
    rolled_min_1 = (
        pd.Series(distances_1)
        .rolling(len(distances_1), min_periods=1)
        .min()
    )
    rolled_max_1 = (
        pd.Series(distances_1)
        .rolling(len(distances_1), min_periods=1)
        .max()
    )

    # normalise the time series values to lies within [0, 1]
    probability_of_crash_1 = (rolled_mean_1 - rolled_min_1) / (
        rolled_max_1 - rolled_min_1
    )
    
    # calculate rolling mean, min, max of homological derivatives
    rolled_mean_2 = pd.Series(distances_2).rolling(20, min_periods=1).mean()
    rolled_min_2 = (
        pd.Series(distances_2)
        .rolling(len(distances_2), min_periods=1)
        .min()
    )
    rolled_max_2 = (
        pd.Series(distances_2)
        .rolling(len(distances_2), min_periods=1)
        .max()
    )

    # normalise the time series values to lies within [0, 1]
    probability_of_crash_2 = (rolled_mean_2 - rolled_min_2) / (
        rolled_max_2 - rolled_min_2
    )

    # define time intervals to plots
    is_date_in_interval = (time_index_derivs > pd.Timestamp(start_date)) & (
        time_index_derivs < pd.Timestamp(end_date)
    )
    probability_of_crash_1_region = probability_of_crash_1[is_date_in_interval]
    probability_of_crash_2_region = probability_of_crash_2[is_date_in_interval]

    time_index_region = time_index_derivs[is_date_in_interval]
    resampled_close_price_region = price_resampled_derivs.loc[is_date_in_interval]

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    plt.plot(
        resampled_close_price_region[probability_of_crash_1_region.values > threshold],
        '#ff7f0e', marker='.', linestyle='None', markersize=4
    )
    plt.plot(
        resampled_close_price_region[probability_of_crash_1_region.values <= threshold],
        "#1f77b4", marker='.', linestyle='None', markersize=4
    )

    plt.title("Baseline Detector")
    plt.ylabel('Close Price', fontsize=12)
    plt.legend(
        [
            "Crash probability > {0}%".format(int(threshold * 100)),
            "Crash probability ≤ {0}%".format(int(threshold * 100)),
        ],
        loc="best",
        prop={"size": 10},
    )

    plt.subplot(1, 2, 2)
    plt.plot(
        resampled_close_price_region[probability_of_crash_2_region.values > threshold],
        '#ff7f0e', marker='.', linestyle='None', markersize=4
    )
    plt.plot(
        resampled_close_price_region[probability_of_crash_2_region.values <= threshold],
        "#1f77b4", marker='.', linestyle='None', markersize=4
    )

    plt.title('Topological Detector')
    plt.legend(
        [
            "Crash probability > {0}%".format(int(threshold * 100)),
            "Crash probability ≤ {0}%".format(int(threshold * 100)),
        ],
        loc="best",
        prop={"size": 10},
    )

    plt.savefig('./images/crash_comparison.png')
    plt.show()

!pip install yfinance



"""# yfinance"""

# Commented out IPython magic to ensure Python compatibility.

# Data source
import yfinance as yf

# Data wrangling
import numpy as np
import pandas as pd

# Data viz
import seaborn as sns
import matplotlib.pyplot as plt
# from plotting import plot_crash_detections, plot_crash_comparisons
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
sns.set(color_codes=True, rc={'figure.figsize':(22,10)})
sns.set_palette(sns.color_palette('muted'))

def getstock(stock_name):
  SP500 = yf.Ticker(stock_name)
  sp500_df = SP500.history(period="max")
  return sp500_df
list=["MNST","TSCO","ODFL","RELI","NVDA","NFLX","AMZN","IDXX","ANSS","ISRG","AAPL","RHHBY","MA","DIS","ORCL","LVMUY","KO","INTC","MO","UNH","BABA","BAC","XOM","HD","JPM"]
for i in list:
  sp500_df=getstock(i)
  price_df = sp500_df['Close']
  # price_df.plot()
  # plt.title('Close Price')
  #  plt.show()
  start_year = '2019'
  price_resampled_df = price_df.resample('24H').pad()[start_year:]
  plt.plot(price_resampled_df)
plt.title('Price')
plt.show()



# use pad to replace missing values by last non-missing value

print(sp500_df.head(5))
df1=sp500_df

"""## runalltwets

## tweets analysis
"""

!pip install vaderSentiment
!pip install pattern

import numpy as np
from textblob import TextBlob
# import vader
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from pattern.web import Twitter
import pandas as pd


twitter = Twitter(language='en') 
list1=[]
for tweet in twitter.search('"Indian stock market"', count=100, cached=False):

    list1.append(tweet.text)
tweett= {
    'tweets':list1
          }
df=pd.DataFrame(tweett)



def getSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity

def getPolarity(text):
    return TextBlob(text).sentiment.polarity

df['Subjectivity'] = df['tweets'].apply(getSubjectivity)
df['Polarity'] = df['tweets'].apply(getPolarity)


def getSIA(text):
    sia = SentimentIntensityAnalyzer()
    sentiment = sia.polarity_scores(text)
    return sentiment

compound = []
neg = []
pos = []
neu = []
# label=[]
SIA = 0

for i in range(0,len(df['tweets'])):
    SIA = getSIA(df['tweets'][i])
    compound.append(SIA['compound'])
    neg.append(SIA['neg'])
    neu.append(SIA['neu'])
    pos.append(SIA['pos'])
#Store the sentiment scores in the df data set
df['Compound'] = compound
df['Negative'] = neg
df[ 'Neutral'] = neu
df[ 'Positive'] = pos
#Show the df data
# df.head(3)
df['Label'] = 0
df.loc[df['Compound'] > 0.4, 'Label'] = 1
df.loc[df['Compound'] < -0.1, 'Label'] = -1
df.head()
print(df.head(5))

"""# train the tweets data

"""

keep_columns = ['Subjectivity','Polarity','Compound','Negative','Neutral', 'Positive', 'Label'] #'Open','High','Low','Volume',
df['pred Label'] = 0
df = df [keep_columns]
df
#Create the feature data set
X = df
X= np.array(X.drop(['Label'], 1))
#Create the target data set
y= np.array(df['Label'])
#Split the data into 80% training and 20% testing data sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state= 0)
#Create and train the model
model = LinearDiscriminantAnalysis().fit(x_train, y_train)
#Show the models predictions
predictions = model.predict(x_test)
predictions
y_test
#Show the model metrics
# print( classification_report (y_test, predictions))

